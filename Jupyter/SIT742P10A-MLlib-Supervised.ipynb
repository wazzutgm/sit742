{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT742P10A-MLlib-Supervised.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.5.5",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "display_name": "Python 3.5",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "1LAn_LblumHO"
      },
      "cell_type": "markdown",
      "source": [
        "# SIT742: Modern Data Science \n",
        "**(Week 10: Data Analytics (III))**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Session 10A - Spark MLlib (3): Supervised Learning\n",
        "\n",
        "\n",
        "Spark has many libraries, namely under MLlib (Machine Learning Library)! Spark allows for quick and easy scalability of practical machine learning!\n",
        "\n",
        "In this lab exercise, you will learn about how to build a Linear Regression Model, a SVM model, and a Logistic Regression Model, also you will learn how to create Classification and Regression DecisionTree and RandomForest Models, as well as how to tune the parameters for each to create more optimal trees and ensembles of trees.\n",
        "\n",
        "## Content\n",
        "\n",
        "\n",
        "\n",
        "### Part 1 Linear Regression\n",
        "\n",
        "\n",
        "### Part 2 Support Vector Machine\n",
        "\n",
        "\n",
        "### Part 3 Logistic Regression\n",
        "\n",
        "\n",
        "### Part 4 Decision Tree (Regression) \n",
        "\n",
        "4.1 maxDepth Parameter\n",
        "\n",
        "4.2 maxBins Parameter\n",
        "\n",
        "4.3 minInstancesPerNode Parameter\n",
        "\n",
        "4.4 minInfoGain Parameter\n",
        "\n",
        "\n",
        "### Part 5 Decision Tree (Classification)\n",
        "\n",
        "### Part 6 Random Forest (Classification)\n",
        "\n",
        "6.1 numTrees Parameter\n",
        "\n",
        "6.2 featureSubsetStrategy Parameter\n",
        "\n",
        "### Part 7 Random Forest (Regression)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "M6EM00qjumHP"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1.Linear Regression\n",
        "\n",
        "\n",
        "Linear regression uses a \"line of best fit\", based on previous data in order to predict future values. There are plenty of model evaluation metrics that can be applied to linear regression. \n",
        "\n",
        "In this lab, we will look at **Mean Squared Error (MSE)**\n",
        "\n",
        "Import the following libraries: \n",
        "\n",
        "\n",
        "*   LabeledPoint\n",
        "*   LinearRegressionWithSGD\n",
        "*   LinearRegressionModel from pyspark.mllib.regression"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3yZuSngxuptC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "conf = SparkConf().setAppName('project1').setMaster('local')\n",
        "sc = SparkContext.getOrCreate(conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aLYscHEFumHQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4z5ncjLmumHS"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to create a <b>RDD of data</b> called <b>rdd_data</b>. That will be done by using the SparkContext (sc) to read in the <b>brain_body_data.csv</b> dataset. Take a look at the dataset so you have a feel for how it's structured."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HDngv2pKumHS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/brain_body_data.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pCqcvbAgumHU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd_data = sc.textFile(\"brain_body_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Crh0HdkSumHV"
      },
      "cell_type": "markdown",
      "source": [
        "Now, run a <b>map function</b> on <b>rdd_data</b>, where the input is a <b>lambda function</b> that is as follows: <i>lambda line: line.split(\",\")</i>. This is so we can split the dataset by commas, since it's a comma-separated value file (CSV). Store this into a variable called <b>split_data</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7pwhIjECumHW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_data = rdd_data.map(lambda line: line.split(\",\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3pkwVjEjumHX"
      },
      "cell_type": "markdown",
      "source": [
        "Next, run the following function that will convert each line in our RDD into a LabeledPoint."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_iUz7CONumHY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labeledParse(line):\n",
        "    return LabeledPoint(line[0], [line[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DAWeQLzeumHZ"
      },
      "cell_type": "markdown",
      "source": [
        "Now, run a <b>map function</b> on <b>split_data</b>, passing in <b>labeledParse</b> as input. Store the output into a variable called <b>reg_data</b>."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7ggzS-LGumHa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reg_data = split_data.map(labeledParse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AYUKRlixumHc"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we will create a variable called <b>linReg_model</b>, which will contain the linear regression model. The model will be made by calling the <b>LinearRegressionWithSGD</b> class and using the <b>.train</b> function with it. The .train function will take in 3 inputs:\n",
        "<ul>\n",
        "    <li>1st: The training data (reg_data in this case)</li>\n",
        "    <li>2nd: The number of iterations, or how many times the regression will run (use iterations=150)</li>\n",
        "    <li>3rd: step used in SGD (use step=0.00001 in this case) </li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "dz22zWSOumHc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linReg_model = LinearRegressionWithSGD.train(reg_data, iterations=150, step=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GomHpPHOumHe"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will create a variable called <b>actualAndPred</b>, which will contain the actual response, along with the predicted response from the model. This will be done by using the <b>map</b> function on <b>reg_data</b>, and passing in:<br> <b>lambda p: (p.label, linReg_model.predict(p.features))</b> as the input."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m_RXdCeaumHf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "actualAndPreds = reg_data.map(lambda p: (p.label, linReg_model.predict(p.features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dy6zPpWVumHg"
      },
      "cell_type": "markdown",
      "source": [
        "We will calculate the Mean Squared Error (MSE) value for the prediction. Run the following code to calculate the MSE. <br> <br> \n",
        "\n",
        "The map function takes the actual value and subtracts it by the predicted value, then \n",
        "squares the result. This is done for each value. <br> <br> \n",
        "\n",
        "Next, the reduce function sums all of the mapped values together. <br> <br>\n",
        "\n",
        "Afterwards, the result is divided by the number of elements that are present in actualAndPreds.\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "f--p2gnlumHh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MSE = actualAndPreds.map(lambda vp : (vp[1] - vp[0])**2).reduce(lambda x, y: x + y) / actualAndPreds.count()\n",
        "print(\"Mean Squared Error = \" + str(MSE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s6H1lmDEumHi"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2.Support Vector Machine (SVM)\n",
        "Support Vector Machines can be used for both **classification and regression** analysis. In our case, we will be using it for classification. Linear SVM in Spark only supports **binary classification**\n",
        "\n",
        "Import the following libraries: <br>\n",
        "<ul>\n",
        "    <li>SVMWithSGD, SVMModel from pyspark.mllib.classification</li>\n",
        "    <li>LabeledPoint from pyspark.mllib.regression</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DNg6VDhWumHj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
        "from pyspark.mllib.regression import LabeledPoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5THKZLKwumHl"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to create a <b>RDD of data</b> called <b>svm_data</b>. That will be done by using the SparkContext (sc) to read in the <b>sample_svm_data.txt</b> dataset, which is a sample dataset that is built-in to Spark. It contains 322 rows of data. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7_C5tpSWumHl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/sample_svm_data.txt'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EJ7rcBVWumHn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm_data = sc.textFile(\"sample_svm_data.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jYQ1N4rTumHo"
      },
      "cell_type": "markdown",
      "source": [
        "For this dataset, it isn't in a format that we need, so we will need the following function to modify it. This function will also create LabeledPoints out of the data, which is necessary to train the SVM Model. Depending on your dataset, the parsing required will differ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "D_VtfZo3umHo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labeledParse(line):\n",
        "    values = [float(x) for x in line.split(' ')]\n",
        "    return LabeledPoint(values[0], values[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gyVLKb50umHq"
      },
      "cell_type": "markdown",
      "source": [
        "This will be applied to <b>svm_data</b> by using the <b>.map</b> function, and passing in the <b>labeledParse function</b>. This will apply the labeledParse function to the entire dataset. Call the output <b>svm_parsed</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1hYMv-jPumHr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm_parsed = svm_data.map(labeledParse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JRi0wWQzumHu"
      },
      "cell_type": "markdown",
      "source": [
        "Now create a SVM model using the <b>SVMWithSGD.train</b> function called <b>svm_model</b>, which requires two inputs:\n",
        "<ul>\n",
        "    <li>1st: The dataset containing the LabeledPoints (<b>svm_parsed</b> in this case)</li>\n",
        "    <li>2nd: The number of iterations the model will run (<b>120</b> in this case)</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JEsnPNLbumHv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm_model = SVMWithSGD.train(svm_parsed, iterations=120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wrq7DeUaumHw"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will create a variable called <b>svm_Labels_Predicts</b>, which will map a tuple containing the label and the prediction. <br>\n",
        "This will be done by using the <b>.map</b> function once again, but on the parsed data, <b>svm_parsed</b>. <br>\n",
        "The input into svm_parsed.map() will be a lambda function: <b>lambda x: (x.label, svm_model.predict(x.features))</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SPJwyPNwumHx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm_Labels_Predicts = svm_parsed.map(lambda x: (x.label, svm_model.predict(x.features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "colab_type": "text",
        "id": "KhFoh-YQumHz"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we will take a look at the training error, called <b>trainingError</b>, which will tell us the accuracy of how well our model did. It will do this by counting the number of incorrect predictions it made, and divide it by the total number of predictions.<br>\n",
        "We will run a <b>.filter</b> on the model we just created, <b>svm_Labels_Predicts</b>, <b>count</b> the output of that with <b>.count()</b>, then <b>divide</b> by the <b>number of elements in svm_parsed</b>. <br> <br>\n",
        "\n",
        "This filter will take a lambda function as input: <b>lambda (v, p): v != p</b>, which just means that the function will look at the predicted value and the labeled value, then see if the prediction matched the label.<br><br>\n",
        "\n",
        "Make sure to add a <b>.count()</b> to the <b>filter</b>, then <b>divide</b> the whole thing by <b>float(svm_parsed.count())</b>\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NKuPOPnSumH0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainingError = svm_Labels_Predicts.filter(lambda xy: xy[0] != xy[1]).count() / float(svm_parsed.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "X-HCV1YpumH1"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, print trainingError, to see the percentage that the model predicted incorrectly."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-1ELJWt2umH2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(trainingError)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a8OYs8CqumH3"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3.Logistic Regression\n",
        "Logistic Regression is a classifier, similar to SVM. Logistic Regression can be used for Binary Classification, which is pretty clear when looking at the diagram above. In the diagram, where are two distinct sections that data resides, which represents a binary classification. <br> <br> In this lab, we will use the same dataset as the one used for SVM, so we can compare the accuracy of both models.\n",
        "\n",
        "Import the following libraries: <br>\n",
        "<ul>\n",
        "    <li>LogisticRegressionWithLBFGS, LogisticRegressionModel from pyspark.mllib.classification</li>\n",
        "    <li>LabeledPoint from pyspark.mllib.regression</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xQYbED6XumH3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
        "from pyspark.mllib.regression import LabeledPoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HjfXAzJVumH5"
      },
      "cell_type": "markdown",
      "source": [
        "Since we are still using the same dataset as in SVM, we will be using the same <b>svm_parsed</b> variable."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vNYW0YCmumH5"
      },
      "cell_type": "markdown",
      "source": [
        "Create a variable called <b>logReg_model</b>, where we <b>train</b> a <b>LogisticRegressionWithLBFGS</b> model by passing in <b>svm_parsed</b>."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F5h_vnNkumH5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "logReg_model = LogisticRegressionWithLBFGS.train(svm_parsed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AHdBrdhBumH7"
      },
      "cell_type": "markdown",
      "source": [
        "Next, create a variable called <b>logReg_Labels_Predicts</b> by <b>mapping</b> the <b>svm_parsed</b> data and passing in the <b>label</b>, along with the <b>logReg_model prediction</b>. This is similar to what we did in the SVM section of the lab."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s6LRvjVfumH7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logReg_Labels_Predicts = svm_parsed.map(lambda p: (p.label, logReg_model.predict(p.features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-5TjnzNTumH9"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we will find the training error, or percentage that the model predicted incorrect. Thids will by done by applying the <b>filter</b> function on <b>logReg_Labels_Predicts</b>. We will pass in a lambda function that will filter for all values that do not equal <b>(lambda (v, p): v != p)</b>, then apply a <b>count()</b> on the filter. This will get the number of incorrect predictions. Now, we need to divide by the total number of predictions, or <b>float(svm_parsed.count())</b>. Store this as <b>trainingError2</b>. Refer to the SVM section if you need a hint."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NumQXj4TumH-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainingError2 = logReg_Labels_Predicts.filter(lambda vp: vp[0] != vp[1]).count() / float(svm_parsed.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Rjv2xtRiumH_"
      },
      "cell_type": "markdown",
      "source": [
        "Now print trainingError2 and trianingError (from the SVM section)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EAFMDkdlumIA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(trainingError2)\n",
        "print(trainingError)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GpB4s_gbumIB"
      },
      "cell_type": "markdown",
      "source": [
        "It seems as though the training error for Logistic Regression is just slightly better than SVM for this case!"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PQE4BxcsumIC"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4.Decision Tree (Regression)\n",
        "\n",
        "Import the following libraries:\n",
        "<ul>\n",
        "    <li>DecisionTree, DecisionTreeModel from pyspark.mllib.tree</li>\n",
        "    <li>MLUtils from pyspark.mllib.util</li>\n",
        "    <li>time</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k1MDW2nOumID",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
        "from pyspark.mllib.util import MLUtils\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uD83Aa6gumIF"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will load in the <b>poker.txt</b> LibSVM file, which is a dataset based on poker hands. Use <b>MLUtils.loadLibSVMFile</b> and pass in the spark context (<b>sc</b>) and the path to the file <b>'resources/poker.txt'</b>. Store this into a variable called <b>regDT_data</b> "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sQ6DEKQnumIF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/poker.txt'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KUvY7_tbumIH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_data = MLUtils.loadLibSVMFile(sc, 'poker.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AbOfqsEyumII"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to split the data into a training dataset (called <b>regDT_train</b>) and testing dataset (called <b>regDT_test</b>). This will be done by running the <b>.randomSplit</b> function on <b>regDT_data</b>. The input into .randomSplit will be <b>[0.7, 0.3]</b>. <br> <br>\n",
        "\n",
        "This will give us a training dataset containing 70% of the data, and a testing dataset containing 30% of the data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nbQau5wOumII",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(regDT_train, regDT_test) = regDT_data.randomSplit([0.7, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W18TVoOvumIJ"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to create the Regression Decision Tree called <b>regDT_model</b>. To instantiate the regressor, use <b>DecisionTree.trainRegressor</b>. We will pass in the following parameters:\n",
        "<ul>\n",
        "    <li>1st: The input data. In our case, we will use <b>regDT_train</b></li>\n",
        "    <li>2nd: The categorical features info. For our dataset, have <b>categoricalFeaturesInfo</b> equal <b>{}</b></li>\n",
        "    <li>3rd: The type of impurity. Since we're dealing with <b>Regression</b>, we will be have <b>impurity</b> set to <b>'variance'</b></li>\n",
        "    <li>4th: The maximum depth of the tree. For now, set <b>maxDepth</b> to <b>5</b>, which is the default value</li>\n",
        "    <li>5th: The maximum number of bins. For now, set <b>maxBins</b> to <b>32</b>, which is the default value</li>\n",
        "    <li>6th: The minimum instances required per node. For now, set <b>minInstancesPerNode</b> to <b>1</b>, which is the default value</li>\n",
        "    <li>7th: The minimum required information gain per node. For now, set <b>minInfoGain</b> to <b>0.0</b>, which is the default value</li>\n",
        "</ul> <br> <br>\n",
        "\n",
        "We will also be timing how long it takes to create the model, so run <b>start = time.time()</b> before creating the model and <b>print(time.time()-start)</b> after the model has been created. <br>\n",
        "<b>Note</b>: The timings differ on run and by computer, therefore some statements throughout the lab may not directly align with the results you get, which is okay! There are many factors that can affect the time output."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OQxnQ_42umIK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "regDT_model = DecisionTree.trainRegressor(regDT_train, categoricalFeaturesInfo={},\n",
        "                                    impurity='variance', maxDepth=5, maxBins=32,\n",
        "                                    minInstancesPerNode=1, minInfoGain=0.0)\n",
        "print (time.time()-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rnLLEJkIumIL"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we want to get the models prediction on the test data, which we will call <b>regDT_pred</b>. We will run <b>.predict</b> on regDT_model, passing in the testing data, <b>regDT_test</b> that is mapped using <b>.map</b> which maps the features by passing in a lambda function (<b>lambda x: x.features</b>)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PvOIA0OLumIM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_pred = regDT_model.predict(regDT_test.map(lambda x: x.features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "77nlJQRSumIO"
      },
      "cell_type": "markdown",
      "source": [
        "Now create a variable called <b>regDT_label_pred</b> which uses a <b>.map</b> on <b>regDT_test</b>. Pass <b>lambda l: l.label</b> into the mapping function. Outside of the mapping function, add a <b>.zip(regDT_pred)</b>. This will merge the label with the prediction</b> "
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "c3sXLl_-umIQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_label_pred = regDT_test.map(lambda l: l.label).zip(regDT_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ikxJNBy6umIR"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will calculate the Mean Squared Error for this prediction, which we will call <b>regDT_MSE</b>. This will equate to <b>regDT_label_pred.map(lambda (v, p): (v - p)**2).sum() / float(regDT_test.count())</b>, which will take the difference of the actual value and the predicted response, square it, and sum that with the rest of the values. Afterwards, it is divided by the total number of values in the testing data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oB4dDjufumIR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_MSE = regDT_label_pred.map(lambda vp: (vp[0] - vp[1])**2).sum() / float(regDT_test.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mLTx088XumIT"
      },
      "cell_type": "markdown",
      "source": [
        "Next, print out the MSE prediction value (<b>str(regDT_MSE)</b>), as well as the learned regression tree model (<b>regDT_model.toDebugString()</b>), so you have an idea of what the tree looks like."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "CaX2jw_PumIU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Test Mean Squared Error = ' + str(regDT_MSE))\n",
        "print('Learned Regression Tree Model: ' + regDT_model.toDebugString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ea3cgIBDumIV"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've created the basic Regression Decision Tree, let's start tuning some parameters! To speed up the process and reduce the amount of code that appears in this notebook, I've made a function that encorporates all of the code above. This way, we can tune the parameters in a single line of code. <br> <br>\n",
        "\n",
        "Read over the code, and it should be apparent what each of the inputs should be. But just to reiterate:\n",
        "<ul>\n",
        "    <li>1st: maxDepthValue is the value for maxDepth (Type:Int, Range: 0 to 30)</li>\n",
        "    <li>2nd: maxBinsValue is the value for maxBins (Type: Int, Range: >= 2)</li>\n",
        "    <li>3rd: minInstancesValue is the value for minInstancesPerNode (Type: Int, Range: >=1)</li>\n",
        "    <li>4th: minInfoGainValue is the value for minInfoGain (Type: Float)</li>\n",
        "    <ul>\n",
        "        <li><b>NOTE</b>: The input for minInfoGain MUST contain a decimal (ex. -3.0, 0.1, etc.) or else you will get an error</li>\n",
        "    </ul>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0vN8CsOCumIW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def regDT_tuner(maxDepthValue, maxBinsValue, minInstancesValue, minInfoGainValue):\n",
        "    start = time.time()\n",
        "    regDT_model = DecisionTree.trainRegressor(regDT_train, categoricalFeaturesInfo={},\n",
        "                                        impurity='variance', maxDepth=maxDepthValue, maxBins=maxBinsValue,\n",
        "                                        minInstancesPerNode=minInstancesValue, minInfoGain=minInfoGainValue)\n",
        "    print (time.time()-start)\n",
        "\n",
        "    regDT_pred = regDT_model.predict(regDT_test.map(lambda x: x.features))\n",
        "    regDT_label_pred = regDT_test.map(lambda l: l.label).zip(regDT_pred)\n",
        "    regDT_MSE = regDT_label_pred.map(lambda vp: (vp[0] - vp[1])**2).sum() / float(regDT_test.count())\n",
        "\n",
        "    print('Test Mean Squared Error = ' + str(regDT_MSE))\n",
        "    print('Learned Regression Tree Model: ' + regDT_model.toDebugString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fDbpuPWxumIX"
      },
      "cell_type": "markdown",
      "source": [
        "Start off by re-creating the original tree. That requires the inputs: <b>(5, 32, 1, 0.0)</b> into <b>regDT_tuner</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K_u1fqw3umIY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 1, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4DIXpVNZumIa"
      },
      "cell_type": "markdown",
      "source": [
        "Remember that when we are tuning a specific parameter, that we will keep the other parameters at their original value"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ywCtae0kumIa"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1.maxDepth Parameter\n",
        "\n",
        "Let's start by tuning the **maxDepth**  parameter. Begin by setting it to a lower value, such as <b>1</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0VugUvL3umIb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(1, 32, 1, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wq7psiTIumId"
      },
      "cell_type": "markdown",
      "source": [
        "By decreasing the maxDepth parameter, you can see that the run-time slightly decreased, presenting a smaller tree as well. You may also see a slight increase in the error, which is to be expected since the tree is too small to make accurate predictions."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "93-6Vn6ZumId"
      },
      "cell_type": "markdown",
      "source": [
        "Now try increasing to value of <b>maxDepth</b> to a large number, such as <b>30</b>, which is the maximum value."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTfUX8-iumId",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(30, 32, 1, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mBlGppS3umIf"
      },
      "cell_type": "markdown",
      "source": [
        "With a large value for maxDepth, you can see that the run-time increased greatly, along with the size of the tree. The MSE has increased greatly compared to the original, which is due to overfitting of the training data from having a deep tree."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4vprl6MpumIf"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2.maxBins Parameter\n",
        "\n",
        "Now let's tune the <b>maxBins</b> variable. Start by decreasing the value to 2, to see what the lower end of this value does to the tree."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "1CHwQQVOumIf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 2, 1, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6f3_0Q-kumIg"
      },
      "cell_type": "markdown",
      "source": [
        "Comparing this to the original tree, we can see a small decrease in the training time, but not much of a difference in regards to MSE or the size of the tree."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U0_d_PnkumIi"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the upper end, with a value of 15000"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "w9EadG_IumIi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 15000, 1, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "skWTxRPAumIj"
      },
      "cell_type": "markdown",
      "source": [
        "With a very large maxBin value, we don't see too much of a change in the overall time or in the MSE. The model still has the same depth and nodes, as expected."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tZoMenJNumIk"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.3.minInstancesPerNode parameter\n",
        "\n",
        "Next we will look at tuning the **minInstancesPerNode**  parameter. It starts off at the lowest value of 1, but let's see what happens if we keep increasing the value. Starting off with the value  **100**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EL0Z2tfRumIk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 100, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jp6p7V5RumIm"
      },
      "cell_type": "markdown",
      "source": [
        "With minInstancesPerNode set to 100, we don't see much of a change in time and MSE, but we can see that there are less nodes in the tree. Try now with a value of <b>1000</b>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "uWg6WYMAumIn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 1000, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8cPvqEF7umIp"
      },
      "cell_type": "markdown",
      "source": [
        "With a value of 1000, we may see more of a decrease in the time, but the MSE has also increased a little bit. As well, the number of nodes in the model has decreased once again. Let's take it one step further and try with a value of <b>8000</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R59Jy7lWumIp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 8000, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YPAMV5vNumIq"
      },
      "cell_type": "markdown",
      "source": [
        "With a value of 8000, we may see that the run-time to build the model is starting to decrease a lot more, with only a small increase in MSE compared to when the value was set to 1000. The main difference we see is that the tree has become a lot smaller! This is to be expected since we are tuning a stopping parameter, which determines when the model finishes building."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t2yY8fQ2umIq"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.4.minInfoGain Parameter\n",
        "\n",
        "\n",
        "For the last parameter, we will look at the minInfoGain parameter, which was initially set to 0.0. This value works well with negative values, and is very sensitive with values greater than 0.0. Try setting the value to a low number, such as -100.0"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "MO5AoMhxumIr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 1, -100.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NKO_oEswumIs"
      },
      "cell_type": "markdown",
      "source": [
        "Overall, we don't see much of a change at all to anything. Now try changing the value to 0.0003"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iChMy6OPumIt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regDT_tuner(5, 32, 1, 0.0003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nilMkkjRumIv"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that small values greater than zero can cause drastic changes in how the model looks. Here, we see a small decrease in the training time, and small increase in the MSE value. But now the tree only contains one node in it. The affect of this parameter on the tree is similar to minInstancesPerNode, since they are both stopping parameters."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JpRclTmoumIv"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5.Decision Tree (Classification)\n",
        "\n",
        "Now it's time for you to try it out for yourself! Build a Classification DecisionTree in a similar way that the Regression DecisionTree was built. Please note that you will be using the same dataset in this section (regDT_train, regDT_test), therefore you do not need to re-initialize that section.<br> <br> \n",
        "\n",
        "Try to only reference the above section when you are experiencing a lot of difficulty. This section is mainly for you to apply your learning.\n",
        "\n",
        "For some help with the variables:\n",
        "<ul>\n",
        "    <li><b>numClasses</b>: The number of classes for this dataset is <b>10</b> (parameter doesn't require tuning)</li>\n",
        "    <li><b>categoricalFeaturesInfo</b>: Has a value of <b>{}</b> (parameter doesn't require tuning)</li>\n",
        "    <li><b>impurity</b>: There are two types of impurites you can use -- <b>'gini'</b> or <b>'entropy'</b> <i>(Default: 'gini')</i></li>\n",
        "    <li><b>maxDepth</b>: Values range between <b>0 and 30</b> <i>(Default: 5)</i></li>\n",
        "    <li><b>maxBins</b>: Value ranges between <b>2 and 2147483647</b> (largest value for 32-bits) <i>(Default: 32)</i></li>\n",
        "    <li><b>minInstancesPerNode</b> ranges between <b>1 and 2147483647</b> <i>(Default: 1)</i></li>\n",
        "    <li><b>minInfoGain</b>: Ensure it is a float (has a decimal in the value) <i>(Default: 0.0)</i></li>\n",
        "</ul>\n",
        "\n",
        "When displaying the <b>Training Error</b>, use the following formula and print statement instead of MSE: <br>\n",
        "<b>classDT_error = classDT_label_pred.filter(lambda (v, p): v != p).count() / float(regDT_test.count())</b> <br>\n",
        "<b>print('Test Error = ' + str(classDT_error))</b>\n",
        "\n",
        "\n",
        "#### The Goal\n",
        "Try to create a model that is better than the model with default values. Challenge yourself by trying to create the best model you can!\n",
        "\n",
        "\n",
        "#### Note\n",
        "We want a model that doesn't take too long to train and will cause overfitting. Remember that a very large model with high accuracy but long run time may not be good because the model may have overfit the data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "khahYa_7umIw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "classDT_model = DecisionTree.trainClassifier(regDT_train, numClasses = 10, \n",
        "                                     categoricalFeaturesInfo = {},\n",
        "                                     impurity = 'gini', maxDepth = 9,\n",
        "                                     maxBins = 25, minInstancesPerNode = 4,\n",
        "                                     minInfoGain = -3.0)\n",
        "print(time.time() - start)\n",
        "# Evaluate model on test instances and compute test error\n",
        "classDT_pred = classDT_model.predict(regDT_test.map(lambda x: x.features))\n",
        "classDT_label_pred = regDT_test.map(lambda lp: lp.label).zip(classDT_pred)\n",
        "classDT_error = classDT_label_pred.filter(lambda vp: vp[0] != vp[1]).count() / float(regDT_test.count())\n",
        "print('Test Error = ' + str(classDT_error))\n",
        "print('Learned classification tree model:' + classDT_model.toDebugString())\n",
        "\n",
        "# 1.16329193115\n",
        "# Test Error = 0.495765559887\n",
        "# Learned classification tree model:DecisionTreeModel classifier of depth 5 with 63 nodes\n",
        "# Impurity: entropy\n",
        "# maxDepth: 5\n",
        "# maxBins: 32\n",
        "# minInstancesPerNode: 1\n",
        "# minInfoGain: 0.0\n",
        "\n",
        "# 1.16743922234\n",
        "# Test Error = 0.453958865439\n",
        "# Learned classification tree model:DecisionTreeModel classifier of depth 9 with 577 nodes\n",
        "# Impurity: gini\n",
        "# maxDepth: 9\n",
        "# maxBins: 25\n",
        "# minInstancesPerNode: 4\n",
        "# minInfoGain: -3.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lZwhIUJlumIx"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 6.RandomForest (Classifier)\n",
        "\n",
        "Now that we've run through the DecisionTree model, let's work with RandomForests now. The process for this will be similar with the DecisionTree section.\n",
        "\n",
        "Import the following libraries:\n",
        "<ul>\n",
        "    <li>RandomForest, RandomForestModel from pyspark.mllib.tree</li>\n",
        "    <li>MLUtils from pyspark.mllib.util</li>\n",
        "    <li>time</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jVfusX7-umIy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
        "from pyspark.mllib.util import MLUtils\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n45WxAdrumIz"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will load in the <b>pendigits.txt</b> LibSVM file, which is a dataset based on Pen-Based Recognition of Handwritten Digits. Use <b>MLUtils.loadLibSVMFile</b> and pass in the spark context (<b>sc</b>) and the path to the file <b>'resources/pendigits.txt'</b>. Store this into a variable called <b>classRF_data</b> <br> <br>\n",
        "\n",
        "Note: You can also try out this section with the poker.txt dataset if you want to compare results from both sections!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n_40DMmXumI0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/pendigits.txt'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tKYT41eFumI1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "colab_type": "code",
        "id": "djqjb_nyumI2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_data = MLUtils.loadLibSVMFile(sc, 'pendigits.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DVPFWkzCumI3"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to split the data into a training dataset (called <b>classRF_train</b>) and testing dataset (called <b>classRF_test</b>). This will be done by running the <b>.randomSplit</b> function on <b>classRF_data</b>. The input into .randomSplit will be <b>[0.7, 0.3]</b>. <br> <br>\n",
        "\n",
        "This will give us a training dataset containing 70% of the data, and a testing dataset containing 30% of the data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PruA-5C5umI4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(classRF_train, classRF_test) = classRF_data.randomSplit([0.7, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yKMZDXHDumI5"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to create the Random Forest Classifier called <b>classRF_model</b>. To instantiate the classifier, use <b>RandomForest.trainClassifier</b>. We will pass in the following parameters:\n",
        "<ul>\n",
        "    <li>1st: The input data. In our case, we will use <b>classRF_train</b></li>\n",
        "    <li>2nd: The number of classes. For this dataset, there will be 10 classes, so set <b>numClasses</b> equal to <b>10</b>\n",
        "    <li>3rd: The categorical features info. For our dataset, have <b>categoricalFeaturesInfo</b> equal <b>{}</b></li>\n",
        "    <li>4th: The number of trees. We will set <b>numTrees = 3</b>\n",
        "    <li>5th: The feature Subset Strategy. There are various inputs for this parameter, but for the sake of this section we will set <b>featureSubsetStrategy</b> equal to <b>\"auto\"</b></li>\n",
        "    <li>6th: The type of impurity. Since we're dealing with <b>Classification</b>, we will be have <b>impurity</b> set to <b>'gini'</b></li>\n",
        "    <li>7th: The maximum depth of the tree. For now, set <b>maxDepth</b> to <b>5</b>, which is the default value</li>\n",
        "    <li>8th: The maximum number of bins. For now, set <b>maxBins</b> to <b>32</b>, which is the default value</li>\n",
        "    <li>9th: The seed to generate random data. For now, set <b>seed</b> to <b>None</b></li>\n",
        "</ul> <br> <br>\n",
        "\n",
        "We will also be timing how long it takes to create the model, so run <b>start = time.time()</b> before creating the model and <b>print(time.time()-start)</b> after the model has been created. <br>\n",
        "<b>Note</b>: The timings differ on run and by computer, therefore some statements throughout the lab may not directly align with the results you get, which is okay! There are many factors that can affect the time output."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "imRiPDadumI5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "classRF_model = RandomForest.trainClassifier(classRF_train, numClasses = 10, categoricalFeaturesInfo={},\n",
        "                                           featureSubsetStrategy=\"auto\", numTrees=3,\n",
        "                                           impurity='gini', maxDepth=4, maxBins=32, seed=None)\n",
        "print (time.time()-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "As0kleBcumI6"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we want to get the models prediction on the test data, which we will call <b>classRF_pred</b>. We will run <b>.predict</b> on classRF_model, passing in the testing data, <b>classRF_test</b> that is mapped using <b>.map</b> which maps the features using a lambda function (<b>lambda x: x.features</b>)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1RLonDoEumI7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_pred = classRF_model.predict(classRF_test.map(lambda x: x.features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7FT3XEcEumI_"
      },
      "cell_type": "markdown",
      "source": [
        "Now create a variable called <b>classRF_label_pred</b> which uses a <b>.map</b> on <b>classRF_test</b>. Pass <b>lambda l: l.label</b> into the mapping function. Outside of the mapping function, add a <b>.zip(classRF_pred)</b>. This will merge the label with the prediction</b> "
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "OS5uJXsvumI_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_label_pred = classRF_test.map(lambda l: l.label).zip(classRF_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qirG_2ITumJA"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will calculate the Test Error for this prediction, which we will call <b>classRF_error</b>. This will equate to <b>classRF_label_pred.filter(lambda (v, p): v != p).count() / float(classRF_test.count())</b>, which will count the number of incorrectly predicted values and divide it by the total number of predictions."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KqQMNRaKumJA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_error = classRF_label_pred.filter(lambda vp: vp[0] != vp[1]).count() / float(classRF_test.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NuQw7_2oumJD"
      },
      "cell_type": "markdown",
      "source": [
        "Next, print out the test error value (<b>str(classRF_error)</b>, as well as the learned regression tree model (<b>classRF_model.toDebugString()</b>), so you have an idea of what the ensemble looks like."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "MRWQF5t-umJD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Test Error = ' + str(classRF_error))\n",
        "print('Learned classification tree model:' + classRF_model.toDebugString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Mmw8wherumJE"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've created the basic Classification Random Forest, let's start tuning some parameters! This is similar to the previous section, but since most of the tuning parameters have been covered in the Decision Tree section, there will only be two parameter to tune in this section. <br> <br>\n",
        "\n",
        "Read over the code and understand how to build the Classification Random Forest as a whole. For the inputs, we have:\n",
        "<ul>\n",
        "    <li>1st: numTreesValue is the value for numTrees (Type: Int, Range: > 0, Default: 3)</li>\n",
        "    <li>2nd: featureSubsetStrategyValue is the value for featureSubsetStrategyValue (Default: \"auto\")</li>\n",
        "    <ul>\n",
        "        <li>Values include: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\"</li>\n",
        "    </ul>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FjxL8IMoumJE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classRF_tuner(numTreesValue, featureSubsetStrategyValue):\n",
        "    start = time.time()\n",
        "    classRF_model = RandomForest.trainClassifier(classRF_train, numClasses = 10, categoricalFeaturesInfo={},\n",
        "                                           featureSubsetStrategy=featureSubsetStrategyValue, numTrees=numTreesValue,\n",
        "                                           impurity='gini', maxDepth=4, maxBins=32, seed=None)\n",
        "    print (time.time()-start)\n",
        "\n",
        "    classRF_pred = classRF_model.predict(classRF_test.map(lambda x: x.features))\n",
        "    classRF_label_pred = classRF_test.map(lambda l: l.label).zip(classRF_pred)\n",
        "    classRF_error = classRF_label_pred.filter(lambda vp: vp[1] != vp[0]).count() / float(classRF_test.count())\n",
        "    \n",
        "    print('Test Error = ' + str(classRF_error))\n",
        "    print('Learned classification tree model:' + classRF_model.toDebugString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S8xw4SBCumJF"
      },
      "cell_type": "markdown",
      "source": [
        "Start off by re-creating the original Random Forest. That requires the input: <b>(3)</b> and <b>\"auto\"</b> into <b>classRF_tuner</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HgJAtryZumJG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(3, \"auto\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bYQScuDoumJG"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1.numTrees Parameter\n",
        "\n",
        "\n",
        "Let's start by tuning the <b>numTrees</b> parameter. Begin by setting it to a lower value, such as <b>1</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0hZa3l4MumJH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(1, \"auto\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5IUdqeDOumJH"
      },
      "cell_type": "markdown",
      "source": [
        "By setting numTrees to a value of 1, we see a slightly higher test error. Note that with numTrees equal to 1, the classifier acts as a Decision Tree, since there is only one tree in the ensemble."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R5Vc1u_1umJI"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try setting it to a numTrees to a larger value, such as 180. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yBbRGcTqumJI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(180, \"auto\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yTP_QFYPumJJ"
      },
      "cell_type": "markdown",
      "source": [
        "With a lot more trees in the ensemble, the training error has decreased a lot! But the training time has increased substantially as well. Remember that the training time increases roughly linearly with the number of trees."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GjzBC3aCumJJ"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2.featureSubsetStrategy Parameter\n",
        "\n",
        "Remember that the featureSubsetStrategy parameter only changes the number of features used as candidates for splitting. The default is set to <b>\"auto\"</b>, which will select \"all\", \"sqrt\", or \"onethird\" based on the value of numTrees. Since we are basing our analysis off of the default values, we have a numTrees value of 3, which means \"sqrt\" is selected. So let's start by changing it it <b>\"all\"</b>, which will use all of the features"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "evQ_Tl4uumJJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(3, \"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lAQzRWmMumJM"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that there is a small increase in the building time of the model, which is expected since we are considering all of the features. As well, there is a small increase in the test error. A possibility to the increase in test error is that there are some features that aren't \"good\" in the model, causing an increase in the test error. Next, we will try with <b>\"sqrt\"</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rFjM5QoeumJM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(3, \"sqrt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4QlLEkrnumJN"
      },
      "cell_type": "markdown",
      "source": [
        "This has very similar values to the \"auto\", which is correct since \"auto\" is using \"sqrt\" for featureSubsetStrategy, since our numTrees value was set to 3. Let's try using \"onethird\" now, which uses one third of the features."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IBDZ6MZ2umJN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(3, \"onethird\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wyhUSU1PumJP"
      },
      "cell_type": "markdown",
      "source": [
        "We see that the run-time is similar to the default, but the testing error has decreased a little bit. It's possible that there is about the same number of features when you take one third of them, as if you take the square root of them for this particular dataset. Let's try with the last type, which is <b>\"log2\"</b>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "spSfOHyNumJP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classRF_tuner(3, \"log2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ke1UD6h0umJQ"
      },
      "cell_type": "markdown",
      "source": [
        "When using <b>\"log2\"</b>, there is a decrease in run-time, along with testing error!"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CbunHKjrumJQ"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 7.RandomForest (Regression)\n",
        "\n",
        "Now it's time for you to try it out for yourself! Build a Regression RandomForest in a similar way that the Classification RandomForest was built. Please note that you will be using the same dataset in this section (classRF_train, classRF_test), therefore you do not need to re-initialize that section.<br> <br> \n",
        "\n",
        "Try to only reference the above section when you are experiencing a lot of difficulty. This section is mainly for you to apply your learning.\n",
        "\n",
        "For some help with the variables:\n",
        "<ul>\n",
        "    <li><b>categoricalFeaturesInfo</b>: Has a value of <b>{}</b> (parameter doesn't require tuning)</li>\n",
        "    <li><b>featureSubsetStrategy</b>: Can change these values between <b>\"auto\"</b>, <b>\"all\"</b>, <b>\"sqrt\"</b>, <b>\"log2\"</b>, and <b>\"onethird\"</b></li>\n",
        "    <li><b>numTrees</b>: Values range from <b>1</b> to infinity<i>(Default: 3)</i></li>\n",
        "    <ul>\n",
        "        <li>Note: If the value is too large, the system can run out of memory and not run.</li>\n",
        "    </ul>\n",
        "    <li><b>impurity</b>: For Regression, the value must be set to <b>'variance'</b> <i>(Default: 'variance')</i></li>\n",
        "    <li><b>maxDepth</b>: Values range between <b>0 and 30</b> <i>(Default: 5)</i></li>\n",
        "    <li><b>maxBins</b>: Value ranges between <b>2 and 2147483647</b> (largest value for 32-bits) <i>(Default: 32)</i></li>\n",
        "    <li><b>seed</b> Can be set to any value, or to a value based on system time with <i>None</i> <i>(Default: None)</i></li>\n",
        "</ul>\n",
        "\n",
        "When displaying the <b>Mean Squared Error</b>, use the following formula and print statement instead of Training Error: <br>\n",
        "<b>regRF_MSE = regRF_label_pred.map(lambda (v, p): (v - p)**2).sum() / float(classRF_test.count())</b> <br>\n",
        "<b>print('Test Error = ' + str(regRF_MSE))</b>\n",
        "\n",
        "#### The Goal\n",
        "Try to create a model that is better than the model with default values.\n",
        "\n",
        "#### Try to beat!\n",
        "With some parameter tuning, I was able to get a run-time increase of the model by ~0.9 seconds and a Test error decrease of ~2.54. Try to get a value similar to this, or better.\n",
        "\n",
        "#### Note\n",
        "We want a model that doesn't take too long to train and will cause overfitting. Remember that a very large model with high accuracy but long run time may not be good because the model may have overfit the data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w9q0vHkQumJQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "regRF_model = RandomForest.trainRegressor(classRF_train, categoricalFeaturesInfo={},\n",
        "                                    numTrees=14, featureSubsetStrategy=\"onethird\",\n",
        "                                    impurity='variance', maxDepth=11, maxBins=24, seed=None)\n",
        "print(time.time() - start)\n",
        "# Evaluate model on test instances and compute test error\n",
        "regRF_pred = regRF_model.predict(classRF_train.map(lambda x: x.features))\n",
        "regRF_label_pred = classRF_train.map(lambda lp: lp.label).zip(regRF_pred)\n",
        "regRF_MSE = regRF_label_pred.map(lambda vp: (vp[0] - vp[1]) ** 2).sum()/\\\n",
        "                                   float(classRF_train.count())\n",
        "print('Test Mean Squared Error = ' + str(regRF_MSE))\n",
        "print('Learned regression forest model: ' + regRF_model.toDebugString())\n",
        "\n",
        "# 0.541887044907\n",
        "# Test Mean Squared Error = 2.63255831252\n",
        "# Learned regression forest model: TreeEnsembleModel regressor with 3 trees\n",
        "# numTrees: 3\n",
        "# featureSubsetStrategy=\"auto\"\n",
        "# Impurity: variance\n",
        "# maxDepth: 4\n",
        "# maxBins: 32\n",
        "\n",
        "\n",
        "# 1.41001796722\n",
        "# Test Mean Squared Error = 0.088487863674\n",
        "# Learned regression forest model: TreeEnsembleModel regressor with 14 trees\n",
        "# numTrees: 14\n",
        "# featureSubsetStrategy=\"onethird\"\n",
        "# Impurity: variance\n",
        "# maxDepth: 11\n",
        "# maxBins: 16"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}