{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 04: Text Analysis)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n## Session 4A - The Fundamentals of Text Pre-processing\n\nTable of Content\n\n* Part 1. Accessing Various Text Resources\n* Part 2. Basic Steps of Pre-Processing Text \n* Part 3. Summary\n* Part 4. Reading Materials\n\n\n---\n\nThe majority of text data that appears in everyday sources such as books, \nnewspapers, magazines, emails, blogs, and tweets \nis free language text. Given the amount of information stored as text on the Internet, it is not feasible for a human to manually explore such a large amount of text data to extract useful information. Therefore, we have to use automatic approaches, such as text analysis algorithms developed in the fields of text mining, natural language process (NLP) and information retrieval (IR). It is worth knowing that computers cannot directly understand text like humans. For example, humans can automatically break down sentences into units of meaning, but computers cannot. Therefore, text data must be processed before various text analysis algorithms can use it.\n\nUnlike the data you can retrieve from relational databases, text data always appears in an unstructured form.\nBy unstructured we mean that text data exists \"in the wild\" and has not been converted into a structured format, like a spreadsheet. Therefore, it has to be manipulated and converted into a proper structured and numerical format consumable by text analysis algorithms, which is referred to as text pre-processing. It is an important task and a critical step in text analysis. The characters, words and sentences identified by text pre-processing are the fundamental units passed to all the downstream text analysis algorithms, such as part-of-speech tagging, parsing, document classification and clustering, etc.\nThis chapter describes the basic pre-processing steps that are needed to convert unstructured text into a structured \nformat.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "Ht8cT-Ou6a7R"
            }
        }, 
        {
            "source": "## Part 1. Accessing Various Text Resources\n\nWhat are the text corpora and lexical resources often used in text analysis? Where and how can we \naccess them? \nText data used for different text analysis tasks can be derived from various resources, such as \n* **Existing data repositories**, most of which contains corpora that have been either pre-processed into a specific format that can be directly digested by the downstream text analysis algorithms or manually annotated. \nFor example,\n    *  [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table) contains 30 corpora that can be used in text mining tasks, such as regression, clustering, and classification.  \n    * [Linguistic Data Consortium](https://www.ldc.upenn.edu/) contains corpora mainly used in various natural language processing tasks, such as parsing, acoustic analysis, phonological analysis and etc. One disadvantage of using LDC is that its corpora are not free. Users have to buy a license in order to use those corpora.\n    \n* **NLTK**: A language toolkit that also includes a diverse set of corpora and lexical resources, which include, for example,\n    * Plain text corpora, e.g.,\n        * The Gutenberg Corpus contains thousands of books.\n    * Tagged Corpora, e.g.,\n        * The Brown Corpus is annotated with part-of-speech tags. Each word is now paired with its part-of-speech tag.\n           You can retrieve words as (word, tag) tuples, rather than just bare word strings.\n    * Chunked Corpora, e.g.,\n        * The CoNLL corpora includes phrasal chunks (CoNLL 2000), named entity chunks (CoNLL 2002).\n    * Parsed Corpora, e.g.,\n        * The Treebank corpora provide a syntactic parse for each sentence, like the Penn Treebank based on Wall Street Journal samples.\n    * Word List and Lexicons, e.g.,\n        * [WordNet](https://wordnet.princeton.edu/): a large lexical database of English, where nouns, verbs, adjectives and adverbs are organized into interlinked synsets (i.e., sets of synonyms)\n    * Categorized Corpora: \n        * The Reuters corpus: a corpus of Reuters News stories for used in developing text analysis algorithms.\n        \n* **Web**: The largest source for getting text data is the Web. Text can be extracted from webpages or be retrieved\nvia various APIs. For example,\n     * **Wikipedia articles**: The Wikimedia website provides links to download dumps of Wikipedia articles. Click [here](https://dumps.wikimedia.org/enwiki) to view various dumps for English Wikipedia articles. \n     * **Tweets** that allows people to communicate with short, 140-characters messages. It is fortunate that Twitter provides quite well documented API that we can use to retrieve tweets of our interest.\n     * The other text data can be scraped from the Internet, like webpages. Here is a <a href=\"https://www.youtube.com/watch?v=3xQTJi2tqgk\">Youtube video</a> on **scraping websites with Python**.\n\nThe set of NLTK corpora can be easily accessed with interfaces offered by NLTK. Here we show you how to install the text data that comes with NLTK and all the packages included in NLTK.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "et29QNJy6a7X"
            }
        }, 
        {
            "source": "import nltk \n#If you're unsure of which data/model you need, you can start out with the basic list of data + models with:\n#It will download a list of \"popular\" resources, these includes:\nnltk.download(\"popular\")\n#It will download a list of \"retuters\" resources, thses includes:\nnltk.download(\"reuters\")\n#While you downliad the nltk package, it will show the Download path,(root/nltk_data)\n#It will also show the 1st item in the nltk.data.path list\n\n# Specifies the file stored in the NLTK data package at *path*. NLTK will search for these files in the directories specified by ``nltk.data.path``.\nnltk.data.path", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "FyjDKH_y6a7b", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import nltk \n#A new window should open, showing the NLTK Downloader.\n#You can input the related character for the command.\n#For example, if you would like to check the current NLTK confiuation details.\n#Just input 'c' and then input 'd' in the new line.\nnltk.download()", 
            "cell_type": "code", 
            "metadata": {
                "outputId": "e11fda80-5c7b-458b-8ea4-8864c3a1aca4", 
                "colab_type": "code", 
                "id": "WyTNm7Ge6a7m", 
                "colab": {
                    "base_uri": "https://localhost:8080/", 
                    "height": 1729
                }, 
                "executionInfo": {
                    "status": "error", 
                    "timestamp": 1552971128623, 
                    "user_tz": -660, 
                    "user": {
                        "userId": "02817931634636469244", 
                        "displayName": "LIU ZHI", 
                        "photoUrl": ""
                    }, 
                    "elapsed": 15346
                }
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You also can install the NLTK software on the Mac or Windows OS, For example, if you use the Mac OS, then you run the above block's two commands and it will locally gives you a window that looks like the following screenshot. For this lab, we use the Google Colab, a Linux-like system, it will show the command line interface (CLL) not a windows as below if you run the nltk.download() function.\n\n![NLTK](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/nltkInstallWindow.png \"NLTK\")\n\n\n\nThis window,  'NTLK Download', shown on the Mac OS will allows you to browse the available corpora and packages included in NLTK. The Collections tab on the downloader shows how the packages are grouped into sets. You can select the line labeled \"all\" and click \"download\" to obtain all corpora and packages (<font color = \"red\">Warning: the size is a couple of GBs</font>). It will take a couple of minutes to download the corpora and packages, depending on how fast your Internet connection is. You can also choose to just install the copora and packages as you go.\n* * *", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "zC1Msn826a7w"
            }
        }, 
        {
            "source": "## Part 2. Basic Steps of Pre-Processing Text\n\nThe possible steps of text pre-processing are nearly the same for all text analysis tasks, though which pre-processing steps are chosen depends on the specific task. The basic steps are as follows:\n* Tokenization\n* Case normalization\n* Removing Stop words\n* Stemming and Lemmatization\n* Sentence Segmentation\n\nWe will walk you through each of these steps with some examples. First, you need to \ndecide <font color=\"red\">the scope of the text to be used in the downstream text analysis tasks</font>. Should you use an entire document?\nOr should you break the document down into sections, paragraphs, or sentences. Choosing \nthe proper scope depends on the goals of the analysis task.\nFor example, you might choose to use an entire document in document classification and clustering tasks\nwhile you might choose smaller units like paragraphs or sentences in document summarization and information\nretrieval tasks. The scope chosen by you will have an impact on the steps needed in the pre-processing process.\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "v98Jv-IM6a7z"
            }
        }, 
        {
            "source": "### 2.1. Tokenization\n\nText is usually represented as sequences of characters by computers. \nHowever, most natural language processing (NLP) and text mining tasks\n(e.g., parsing, information extraction, machine translation, document classification, information\nretrieval, etc.) need to operate on tokens. \nThe process of breaking a stream of text into tokens is often referred to as **tokenization**.\nFor example, a tokenizer turns a string such as \n```\n    A data wrangler is the person performing the wrangling tasks.\n```\ninto a sequence of tokens such as\n```\n    \"A\" \"data\" \"wrangler\" \"is\" \"the\" \"person\" \"performing\" \"the\" \"wrangling\" \"tasks\"\n```\n\nThere is no single right way to do tokenization. \nIt completely depends on the corpus and the text analysis task you are going to perform. It is important to ensure that your tokenizer produces proper token types for your downstream text analysis tools. \nAlthough word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\nIn this section, we will demonstrate the process of chopping character sequences into pieces with different tokenizers. \n\nThe major question of the tokenization phase is what counts as a token.\nDifferent linguistic analyses might have different notions of tokens.\nIn different languages, a token could mean different things. \nHere we are not going to dive into the linguistic aspect of what counts as a token,\nas it goes beyond the scope of this unit.\nWe rather consider English text.\n**In English, a token can be a string of alphanumeric characters separated by spaces, which\nseems quite easy.**\nHowever, things get considerably worse when we start considering words having\nhyphens, apostrophes, periods and so on. In a word tokenization task, should we\nremove hyphens? Should we keep periods? \nAccording to different text analysis tasks, \ntokens can be unigram words, multi-word phrases (or collocations), or \nother meaningful and identifiable linguistic elements.\nTherefore, working out word tokens is not an easy task in pre-processing natural language text.\nYou might be interested in watching a YouTube video on [word tokenization](https://www.youtube.com/watch?v=f9o514a-kuc).", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "jR2B1MS_K86m"
            }
        }, 
        {
            "source": "raw = \"\"\"The GSO finace group in  U.S.A. provided Cole with about\nUS$40,000,555.4 in funding, which accounts for 35.3% of Cole's revenue (i.e., AUD113.3m), \nas the ASX-listed firm battles for its survival.\nMr. Johnson said GSO's recapitalisation meant \"the current shares are worthless\".\"\"\"", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "i7sE3bDa6a72", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### 2.1.1 Standard Tokenizer\n\nFor English, a straightforward tokenization strategy is to use white spaces as token delimiters. \nThe whitespace tokenizer simply splits the text on any sequence of whitespace, tab, or newline characters.\nConsider the above hypothetical text.\nAs a starting point, let's tokenize the text above by using any whitespace characters as token delimiters.\nAs mentioned, these characters include whitespace (' '), tab ('\\t'), newline ('\\n'), return ('\\r'), and so on.\nYou have learnt in week 2 that those characters are together represented by a built-in regular expression abbreviation '\\s'.\nThus, we will use '\\s' rather than writing it as something like '[ \\t\\n]+'.\nYou can read the details about the [\"\\s\" Syntax](https://docs.python.org/3/library/re.html)\n\nThere are multiple ways of tokenizing a string with whitespaces.\nThe simplest approach might be using Python's string function `split()`.\nThis function returns a list of tokens in the string.\nAnother way is to use Python's regular expression package, `re` as\n```python\n    import re\n    re.split(r\"\\s+\", raw)\n```\nThe output should be exactly the same as that given by the string function `split()`.\nHere we further demonstrate the use of <font color=\"blue\">RegexpTokenzier</font> from Natural Language Toolkit (NLTK).", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "r2yfyRDj6a8A"
            }
        }, 
        {
            "source": "from nltk.tokenize import RegexpTokenizer", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "yKv-cbC96a8G", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#For the RegexpTokenizer function, the arguement gaps type is bool.\n#we will use the 'True' if this tokenizer's pattern should be used to find separators between tokens; \n#we will use the 'False' if this tokenizer's pattern should be used to find the tokens themselves.\n\n#The below example with gasp param is True\ntokenizer = RegexpTokenizer(r\"\\s+\", gaps=True)\ntokens = tokenizer.tokenize(raw)\nprint(tokens)\n\n#The below example with gasp param is False\ntokenizer_test = RegexpTokenizer(r\"\\s+\", gaps=False)\ntokens_test = tokenizer_test.tokenize(raw)\nprint(tokens_test)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "QAWtbwiJ6a8R", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A <font color=\"blue\">RegexpTokenizer</font> splits a string into tokens using a regular expression.\nRefer to its online [documentation](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) \nfor more details.\nIts constructor takes four arguments.\nThe compulsory argument is the pattern used to build the tokenizer.\nIt is in the form of a regular expression. \n**In the example above, we used `\\s+` to match 1 or more whitespace characters.**\nIf the pattern defines separators between tokens, the value of `gaps` should be\nset to `True`. Otherwise, the pattern should be used to find the tokens.\nNLTK also provides a whitespace tokenizer, `WhitespaceTokenizer[source]`, which is\nequivalent to our tokenizer. Try\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "dvEgk0LU6a8e"
            }
        }, 
        {
            "source": "from nltk.tokenize import WhitespaceTokenizer\nWhitespaceTokenizer().tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "akOPDrnC-_H_", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It seems that word tokenization is quite simple if words in a language are all\nseparated by whitespace characters. \nHowever, this is not the case in many languages other than English, **such\nas Chinese, Japanese, Korean and Ancient Greek.** \nIn those languages, text is written without any whitespaces between words. \nSo the whitespace tokenizer is of no use at all.\nTo handle them, we need more advanced tokenization techniques, often referred to as\nword segmentation, which is an important and challenging task in NLP. \n**However,\ndiscussing word segmentation is beyond our scope here.**\n\nIt is not surprising that the whitespace tokenizer is **insufficient** even for English, since English does not just contains sequences of alphanumeric characters separated by white spaces. \nIt often contains punctuation, hyphen, apostrophe, and so on.\nSometimes **whitespace does not necessarily indicate a word break. **\nFor example, non-compositional phrases (e.g., \"real estate\" and \"shooting pain\") and proper nouns (e.g., \"The New York Times\") have a different meaning than the sum of their parts. They cannot be split in the process of word tokenization.\nThey must be treated as a whole in, for instance, information retrieval.\n\nBack to our example, \nthe whitespace tokenizer still gives us word like \"(i.e.,\", \"funding,\" and \"worthless\".\".\nWe would like to remove parentheses, some punctuations, quotation marks and other non-alphanumeric characters.\nA simple and straightforward strategy is to use all non-alphanumeric characters as token delimiters.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "JVYonrOs-_xK"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"\\W+\", gaps=True) \ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "Rj9Uo8PG6a8k", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "In regular expressions, '\\W' indicates any non-alphanumeric characters (equivalent to `[^a-zA-Z0-9]`) while '\\w' indicates any alphanumeric characters (equivalent to `[a-zA-Z0-9]`). \nThe counterpart is to extract tokens that only consist of alphanumeric characters without the empty strings. Try the following out yourself:\n```python\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    tokenizer.tokenize(raw)\n```\n\nThese two strategies are simple to implement, but there are cases where they may not match the desired behaviour. \nFor example, the whitespace tokenizer cannot properly handle non-alphanumeric characters, while the non-alphanumeric tokenizer might over-tokenise some tokens with periods, hyphens, apostrophes, etc.\nIn the rest of this section, we will discuss the main problems that you might face while tokenising free language text. You will soon find that tokenizers should often be customized to deal with different datasets.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "xDJ813sU6a82"
            }
        }, 
        {
            "source": "#\\w means match any alphanumberic characters\ntokenizer = RegexpTokenizer(r\"\\w+\", gaps=True) \ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "xaDUmnKoE59l", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### 2.1.2 Periods in Abbreviations\n\nWord tokens are not always surrounded by whitespace characters. Punctuation, such as commas, semicolons, and periods, are often used in English, as they are vital to disambiguate the meaning of sentences. However, it is problematic for computers to handle punctuation, especially periods, properly in tokenization. \nIn this part we will focus on the handling of periods.\n\nPeriods are usually used to mark the end of sentences. Difficulty arises when the period marks abbreviations (including acronyms). Please refer to **\"Step 2: Handling Abbreviations\" in [3]** for a detailed discussion on abbreviations.  In the case of abbreviations, particularly acronyms, separating tokens on punctuation and other non-alphanumeric characters would put different components of the acronym into different tokens, as you have seen in our example, where \"U.S.A\" has been put into three tokens, \"U\", \"S\" and \"A\", losing the meaning of the acronym. To deal with abbreviations, one approach is to maintain a look-up list of known abbreviations during tokenization. Another approach aims for smart tokenization. Here we will show you how to use regular expressions to cover most but not all abbreviations.\n\nAn acronym is often formed from the initial components in multi-word phrases.  Some contains periods, and some do not. Common acronyms with periods are for example, \n* U.S.A\n* U.N.\n* U.K.\n* B.B.C\n\nOther abbreviations with a similar pattern are, for instance, \n* A.M. and P.M.\n* A.D. and B.C.\n* O.K.\n* i.e.\n* e.g.\n\nFor abbreviations like those, it is not hard to figure out the pattern and the corresponding regular expression.  Each of those abbreviations contains at least a pair of a letter (either uppercase or lowercase) and a period.  The regular expression is\n```python\n    r\"([a-zA-Z]\\.)+\"\n```\nTo see the graphical representation of the regular expression, please click the [RegexpTokenizer](https://regexper.com/#%28%5Ba-zA-z%5D%5C.%29%2B) webpage.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "U_1ScDlL6a86"
            }
        }, 
        {
            "source": "#If you directly use the r\"([a-zA-Z])\", you will find out that the output is different with your expect. \ntokenizer = RegexpTokenizer(r\"([a-zA-Z]\\.)+\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "GYOFGHKtG-uy", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Then, we add the ?: in the above regular expression.\ntokenizer = RegexpTokenizer(r\"(?:[a-zA-Z]\\.)+\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "xtdc9DPK6a8-", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Observe that\n1. We introduced <font color=\"red\">(?: )</font> in the regular expression to avoid just selecting substrings that match the pattern. `(?:)` is a non-capturing version of regular parentheses. If the parentheses are used to specify the scope of the pattern, but not to select the matched material to be output, you have to use `(?:)`. To check out how `?:` affects the output, try to remove it and run the tokenizer again. You will get the following output\n```\n    ['e.', 'A.', 'l.', 'r.']\n```\nIt just returns the last substrings that match the pattern.\n2. The code also returned 'l.' and 'r.' that are part of 'survival.' and 'Mr.' \nThe period in 'survival.' marks the end of a sentence. \nIndeed, it is very challenging to deal with the period at the end of each sentence, as it can also be part of an abbreviation if the abbreviation appears at the end of a sentence.\nFor example, the following sentence ends with 'etc.'\n```\n    I need milk, eggs, bread, etc.\n```\n\nNext, let\u2019s further consider some more general abbreviations, like\n* Mr. and Mrs.\n* Dr.\n* st.\n* Wash. and Calif. (abbreviations for two states in U.S., Washington and California)\n\nIn those abbreviations, the period is always preceded two or more letters in English alphabet. Turn this pattern into a regular expression\n```\n    r\"[a-zA-z]{2,}\\.\"\n```", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "YpjaA1Jm6a9I"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"[a-zA-z]{2,}\\.\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "_1gelRs56a9X", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is not surprising that the ouput contains \"survival.\" again. \nThe issue of working out which punctuation marks indicate the end of a setence will be discussed in section 2.5.\nLet's put all the cases together. \nThe regular expression can be generalised to\n```python\n    r\"([a-zA-Z]+\\.)+\"\n```\nwhich matches both acronyms and abbreviations like \"Dr.\"", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "7U3-4wm26a9r"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"([a-zA-z]+\\.)+\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "gZjNqYKeJiZk", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "As we mentioned early in this chapter, the issues of tokenization are language specific.\nThe language of the document to be tokenized should be known a priori.\nTake computer technology as an example.\nIt has introduced new types of character sequences that a tokenizer should probably treat as a single token, including email addresses, web URLs, IP addresses, etc. One solution is to simply ignore them by using a non-alphanumeric-based tokenizer. \nHowever, this comes the cost of losing the original meaning of those kinds of tokens. For instance, if an IP address, like \"172.19.197.106\", is tokenized into individual numbers, \"172\", \"19\", \"197\", and \"106\".\nIt is no longer an IP address, and these numbers can be anything.\nTo account for strings like\n* \"172.19.197.106\"\n* \"www.mit.edu\"\n\nyou can simply update our regular expression accounting for abbreviations to \n```python\n    (\\w+\\.?)+\n```\n\nTry it out on http://regexr.com/.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "jhUyz8cy6a92"
            }
        }, 
        {
            "source": "#Token for mathing IP address\ntokenizer = RegexpTokenizer(r\"\\d{1,3}\")\nprint(tokenizer.tokenize(\"172.19.197.106\"))\n\n#Token for mathing word in a UTL\ntokenizer = RegexpTokenizer(r\"\\w{1,}\")\nprint(tokenizer.tokenize(\"www.mit.edu\"))\n\n#the last word in a IP address or a URL\ntokenizer = RegexpTokenizer(r\"(\\w+\\.?)+\")\nprint(tokenizer.tokenize(\"172.19.197.106\"))\nprint(tokenizer.tokenize(\"www.mit.edu\"))", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "u08JXBR9J-22", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### 2.1.3 Currency and Percentages\n\nWhile analysing financial document, such as finance reports, a financial analyst might be interested in monetary numerals mentioned in the reports. One interesting research question in both finance and computer science is whether one can use finance reports to help predict the stock market prices. In this case, it would be good for a tokenizer to keep all the monetary numerals.\n\nCurrency is usually expressed in symbols and numerals (e.g., $10).\nThere are many different ways of writing about different currencies.\nFor example,\n* A three-letter currency abbreviations followed by figures, for example,\n```\n    AUD100, EUR500, CNY330 \n```\n\n* A letter or letters symbolising the country followed the, for example,\n```\n    A$100 (= AUD100), US$10 (= USD10), C$5 (= CAD5),\n```\n\n* A currency symbols ($, \u00a3, \u20ac, \u00a5, etc.) followed by figures, for examples\n```\n    \u00a3100.5, \u20ac30.0\n```\n\nWhile the number of digits in the integer part is more than three, commas are often inserted between every three digits, like\n```\n    AUD100, 000 \n```\nLet's construct a regular expression that can account for all the following monetary numerals\n```\n1. $10,000.00\n2. \u20ac10,000,000.00\n3. \u00a55.5555\n4. AUD100\n5. A$10.555\n```\nThe regular expression should looks like as follows (<a href=\"https://regexper.com/#(%3F%3A%5BA-Z%5D%7B1%2C3%7D)%3F%5B%5C%24\u00a3\u20ac\u00a5%5D%3F(%3F%3A%5Cd%7B1%2C3%7D%2C)*%5Cd%7B1%2C3%7D(%3F%3A%5C.%5Cd%2B)%3F\"> the graphical representation</a>):\n```python\n    r\" (?:          \n        [A-Z]{1,3})?                 # (1)\n        [\\$\u00a3\u20ac\u00a5]?         # (2)\n        (?:\\d{1,3},)*      # (3)\n        \\d{1,3}          # (4)\n        (?:\\.\\d+)?        # (5)\n   \"\n```\n\n![The diagram for this regular expression](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/P04A01.png)\n\n\n(1) matches the start of monetary numerals, which consists of one or up to 3 uppercase letters that indicate a country symbol or a currency abbreviation.\n<br/>\n(2) together with (1), matches the start of monetary numerals, which consists of either only a currency symbol or a country symbol plus a currency symbol.\n<br/>\n(3) accounts for the integer part that contains more than three digits. It matches all digits in the integer part except for the last three digits.\n<br/>\n(4) matches the last three digits in the integer part.\n<br/>\n(5) matches the fractional part.\n", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "uEujTvvy6a99"
            }
        }, 
        {
            "source": "#Let run the above regular expression\ntokenizer = RegexpTokenizer(r\"(?:[A-Z]{1,3})?[\\$\u00a3\u20ac\u00a5]?(?:\\d{1,3},)*\\d{1,3}(?:\\.\\d+)?\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "Drp_VOFm6a-T", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Refer back to our example text \"raw\", can you find any issue rather than the percentage (35.5%)? The regular expression cannot handle \"AUD113.3m\", where the \"m\" indicates million. Without 'm', the number 'AUD113.3' loses its meaning in the original context. Therefore, you have seen that there might not be a regular expression that can handle all possible ways of representing currency.\n\nNow, we have constructed a regular expression for currencies, even though it is not perfect.\nNext, we move to working out the regular expression for percentages, things becomes quite easy.\nPercentages usually have the following forms\n* 23%\n* 23.23%\n* 23.2323%\n* 100.00%\n\nThe maximum number of digits in the integer part is 3, the minimun is 1, so the regular expression is '\\d{1,3}'.\nA percentage can have either one or no fractional part, which can be matched by '(\\.\\d+)?'.\nAdding % to the end, we have (<a href=\"https://regexper.com/#%5Cd%7B1%2C3%7D(%5C.%5Cd%2B)%25\">the graphical representation</a>)\n```python\n    r\"\\d{1,3}(\\.\\d+)%\"\n```\n\n![The diagram for this regular expression](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/P04A02.png)", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "uXanOxsH6a-n"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"\\d{1,3}(?:\\.\\d+)?%\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "I45vt11t6a-s", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The above code should give you the only percentage in our example text. \nCompare the regular expression matching percentages with that matching currency,\nyou will find that the former is similar to the last bits of the latter, except for the percentage sign.\nBesides, there are other numerical and special expressions that\nwe can not easily handle with regular expressions. For example, these expressions include\nemail addresses, time, vehicle licence numbers, phone numbers, etc.\nIf you are interested in dealing with them, you could read the \u201cRegular Expressions Cookbook\u201d by Jan Goyvaerts and Steven Levithan. ", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "NsfJgax26a-2"
            }
        }, 
        {
            "source": "#### 2.1.4 Hyphens and Apostrophes \n\nIn English, hyphenation is used for various purposes. The hyphen can be used to form certain compound terms, including hyphenated compound nouns, verbs and adjectives. It can also be used for word division. There are many sources of hyphens in texts. Thus, should one count a sequence of letters with a hyphen as one word to two? Unfortunately, the answer seems to be sometimes one, sometimes two. \nFor example, if the hyphen is used to split up vowels in words, such as \"co-operate\", \"co-education\" and \"pre-process\", these words should be regarded as single token. In contrast, if the hyphen is used to group a couple of words together, for example, \"a state-of-the-art algorithm\" and \"a money-back guarantee\", these hyphenated words should be separated into individual words.\nTherefore, handling hyphenated words automatically is one of the most difficult tasks in pre-processing text data.\n\n\"**The Art of Tokenization**\" (Please refer the Part 4, Reading Materials) categorizes different hyphens into three types:\n* **End-of-Line Hyphen**: In professionally printed material (like books, and newspapers), the hyphen is used to divide words between the end of one line and the beginning of the next in order to perform justification of text during typesetting. It seems to be easy to handle these kinds of hyphens by simply removing them and joining the parts of a word at the end of one line and the beginning of the next.\n* **Lexical Hyphen**: Words with a lexical hyphen are better to be treated as a single word. They are typically included in a dictionary. For example, words contains certain prefixes, like \"co-\", \"pre-\", \"multi-\", etc., and other words like \"so-called\", \"forty-two\"\n* **Sententially Determined Hyphenation**: This type of hyphen is often created dynamically. It includes, for example, nouns modified by an 'ed'-verb (e.g., \"text-based\" and \"hand-made\") and sequences of words used as a modifier in a noun group, as in \"the 50-cent-an-hour raise\". In these cases, we might want to treat those tokens joined by hyphens as individual words.\n\nThe use of hyphens in many such cases is extremely inconsistent, which further increase the complexity of dealing with hyphens in tokenization. People often resort to using either some heuristic rules or treating it as a machine learning problem. However, these go beyond our scope here. It is clear that handling hyphenation is much more complicated than one can expect. You should also be clear that there is no way of handling all the cases above.\n\nLet's assume that we are going to treat all strings of two words separated by a hyphen as a single token, how can we extract them from texts without breaking them into pieces.  In our example text, we are going to view \"ASX-listed\" as a single token. The pattern here is  a sequence of alphanumeric character plus \"-\" and plus another sequence of alphanumeric character.\nThe corresponding regular expressions should be \n```python\n    r\"\\w+-\\w\"\n```", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "hngoWmJi6a-5"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"\\w+-\\w+\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "I9rchg8I6a-_", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Similar to hyphens, how to handle an apostrophe in tokenization is another complex question. The apostrophe in English is often used in two cases:\n* Contractions: a shortened version of a word or multiple words. \n    * don't (do not)\n    * she'll (she will)\n    * you're (you are)\n    * he's (he is or he has)\n    * you'd (you would)\n* Possessives: used to indicate ownership/possession with nouns.\n    * the cat's tail\n    * Einstein's theory\n    \nShould we treat a string containing apostrophes as a single word or two words?\nPerhaps, you might think we should separate English Contractions into two words, and regard possessives as a single word. \nHowever, distinguishing contractions from possessives is not easy.\nFor example, should \"cat's\" be \"cat has/is\" or the possessive case of cat.\nThus some processor in NLP splits the strings in either case into two words, while others do not.\nHere we again assume that we are going to retrieve all strings with an apostrophe as single words.\nThe regular expression is quite similar to the one for handling hyphens.\n```\n     r\"\\w+'\\w+\"\n```", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "FKnqjBWh6a_K"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"\\w+'\\w+\")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "p8lUk62W6a_Y", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now let's generalise the `\\w+` to permit word-internal hyphens and apostrophes (<a href=\"https://regexper.com/#%5Cw%2B(%3F%3A%5B-'%5D%5Cw%2B)%3F\">the graphical representation</a>):\n```python\n    \\w+(?:[-']\\w+)? \n```\n\nYou have learnt some simple approaches for handling different issues in word tokenization, which turns out to be far more difficult than you might have expected. It is clear that different NLP and text mining tasks on different text corpora need different word tokenization strategies, as you must decide what counts as a word. Besides the `RegexpTokenizer`, NLTK implements a set of other word tokenizaton modules. Please refer to [its official webpage](http://www.nltk.org/api/nltk.tokenize.html) for more details.\nSo far that we have only considered well-written text, but there are other types of natural language texts, such the transcripts of speech corpora and some non-standard texts like tweets that provide their own additional challenges.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "e5trx9fP6a_x"
            }
        }, 
        {
            "source": "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)? \")\ntokenizer.tokenize(raw)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "lnhfVL6HR6Hz", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### 2.2. Case Normalization\nAfter word tokenization, you may find that words can contain either upper- or lowercase letters. \nFor example, you might have \"data\" and \"Data\" appearing in the same text.\nShould one treat them as two different words or as the same word?\nMost English texts are written in mixed case. \nIn other words, a text can contain both upper- and lowercase letters.\nCapitalization helps readers differentiate, for example, between nouns and proper nouns.\nIn many circumstances, however, an uppercase word should be treated no differently than in lower case appearing in a document, and even in a corpus.\nTherefore, a common strategy is to reduce all letters in a word to lower case.\nIt is very simple to do so.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "tp9IPKDr6a_5"
            }
        }, 
        {
            "source": "tokens = [token.lower() for token in tokens]\ntokens", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "H-BhfNO36a__", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is often a good idea to do case normalization. For example, with case normalization, you can match \"data wrangling\" with \"Data Wrangling\" in an information retrieval task. But for other tasks, like named entity recognition, one would better to keep capitalised words (e.g., pronouns) left as capitalised.\nPeople have tried some simple heuristics that just makes some token lowercase. \nHowever, there is a trade-off between getting capitalization right and simply using lowercase regardless of the correct case of words.\nYou can read about basic formatting issues of text processing in \"Corpus-Based Work\" on the Part 4, Reading Materials.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "R4hQJMHE6bAP"
            }
        }, 
        {
            "source": "### 2.3. Removing Stop words\n[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are words that are extremely common and carry little lexical content. For many NLP and text mining tasks, it is useful to remove stopwords in order to save storage space \nand speed up processing, and the process of removing these words is usually called \u201cstopping.\u201d \nAn example stopword list from NLTK is shown bellow:", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "gWw6xdFV6bAY"
            }
        }, 
        {
            "source": "from nltk.corpus import stopwords\n\nstopwords_list = stopwords.words('english')\n\n#show the stopword in the 'english' database\nstopwords_list", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "bQc14Mrg6bAx", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The above list contains 127 stopwords in total, which are often [function words](https://en.wikipedia.org/wiki/Function_word) in English, like articles (e.g., \"a\", \"the\", and \"an\"), \npronouns (e.g., \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc.\nIt is easy to use NLTK's built-in stopword list to remove all the stopwords from a tokenised text.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "fdELVGtT6bBR"
            }
        }, 
        {
            "source": "filtered_tokens = [token for token in tokens if token not in stopwords_list]\nfiltered_tokens", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "vOM8t8d_6bBl", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#This will show all exclude stopwords from the filtered list\nexcluded_tokens = [token for token in tokens if token in stopwords_list]\nexcluded_tokens", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "8QTQwnCdTvct", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We have removed 13 stopwords. The rest Token number is 28. \nTo check what stopwords have been excluded from the filtered list, you simply change `not in` to `in`.\n\nThere is no single universal list of stop words used by all NLP and text mining tools.\nDifferent stopword lists are available online. For example, the English stopword list \navailable at [Kevin Bouge's website](https://sites.google.com/site/kevinbouge/stopwords-lists) \nwhich contains 570 stopwords, a quite fine-grained stopword list. \nAt the same website, you can also download stopword lists for 27 languages other than English.\nPlease download the English stopwords list from Kevin Bourge's website, and save it into the folder where\nyou keep this IPython Notebook file. \nWe will try out the aforementioned stopword lists on the large\n[Reuters corpus](https://github.com/teropa/nlp/tree/master/resources/corpora/reuters). ", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "tiG652Wi6bB2"
            }
        }, 
        {
            "source": "!pip install wget\n\nimport wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/stopwords_en.txt'\n\nDataSet = wget.download(link_to_data)\n\n!ls", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "99wYIR5x6bB8", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import nltk\nreuters = nltk.corpus.reuters.words()\n\nstopwords_list_570 = []\nwith open('stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()\n#It will show the retuers stopwords, you can compare it with the above 'english'stopwords.\n#You will find that the 'retuers'stopwords is more abundant than 'english' stopwords. \nstopwords_list_570", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "4KaWJx9R6bCw", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Remove stop words accroding to NLTK's built-in stopword list.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "ixs2WLXN6bDn"
            }
        }, 
        {
            "source": "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list]\n#It will show the percentage between the filtered_retuers and the 'english' stopwords\nlen(filtered_reutuers)*1.0/len(reuters)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "sp4I6HPH6bEB", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Remove stop words according to the downloaded stop word list. (Note: the following script will run a couple of minutes due to data structure used in search.)", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "6QPqNXCo6bEf"
            }
        }, 
        {
            "source": "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list_570]\n#It will show the percentage between the filtered_retuers and the 'retuers' stopwords\n#It will show that the retuers stopwords will filte more stopwords. \nlen(filtered_reutuers)*1.0/len(reuters)", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "CroshJn16bEk", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Thus, with the help of these two stopword lists, we can filter about 36% and 34% of the words respectively.\nWe have significantly reduced the size of the Reuters corpus. \nThe question is: Have we lost lots of information due to removing stopwords? \nFor the large majority of NLP and text mining tasks and algorithms, stopwords usually appear to be of little value and have little impact on the final results, as the presence of stopwords in a text does not really help distinguishing it from other texts. \nIn contrast, text analysis tasks involving phrases are the exception because phrases lose their meaning if some of the words are removed. \nFor example, if the two stopwords in the phrase \"a bed of roses\" are removed, its original meaning in the context of IR will be lost.\n\nStopwords usually refer to the most common words in a language. \nThe general strategy for determining whether a word is a stopword or not is to compute its total number of appearances in a corpus. \nWe will cover more about removing common words other than stopwords while we further explore text data in next chapter.\nHere we would like to point out that failing to remove those common words could lead to skewed analysis results.\nFor example, while analysing emails we usually remove headers (e.g., \"Subject\", \"To\", and \"From\") and sometimes\na lengthy legal disclaimer that often appears in many corporate emails.\nFor short messages, a long disclaimer can overwhelm the actual text when performing any sort of text analysis.\nFor more discussion on stopping, please read [5] and watch an 8-mintue YouTube video on [Stop Words](https://www.youtube.com/watch?v=w36-U-ccajM).", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "Vbc9EF4j6bEx"
            }
        }, 
        {
            "source": "### 2.4. Stemming and Lemmatization\n\nAnother question in text pre-processing is whether we want to keep word forms like \"educate\", \"educated\", \"educating\", \nand \"educates\" separate or to collapse them. Grouping such forms together and working in terms of their base form is \nusually known as stemming or lemmatization.\nTypically the stemming process includes the identification and removal of prefixes, suffixes, and pluralisation, \nand leaves you with a stem.\nLemmatization is a more advanced form of stemming that makes use of, for example, the context surrounding the words, \nan existing vocabulary, morphological analysis of words and other grammatical information (e.g., part-of-speech tags) \nto determine the basic or dictionary form of a word, which is known as the lemma.\nSee Wikipedia entries for [stemming](https://en.wikipedia.org/wiki/Stemming) \nand [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation).\n\nStemming and lemmatization are the basic text pre-processing methods for texts in languages like English, French, \nGerman, etc. \nIn English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are \ninflected in the comparative/superlative. \nFor example,\n* watch &#8594; watches\n* party &#8594; parties\n* carry &#8594; carrying\n* love &#8594; loving\n* stop &#8594; stopped\n* wet &#8594; wetter\n* fat &#8594; fattest\n* die &#8594; dying\n* meet &#8594; meeting\n\nIt is not hard to find that they all follow some inflections rules. \nFor instance, to get the plural forms of nouns endings with consonant 'y', one often changes the ending \n'y' to 'ie' before adding 's'. \nIndeed most existing stemming algorithms make intensive use of this kind of rules.\n\nIn morphology, the derivation process creates a new word out of an existing one often by adding either \na prefix or a suffix. It brings considerable sematic changes to the word, often word class is changed, for example,\n* dark &#8594; darkness\n* agree &#8594; agreement\n* friend &#8594; friendship\n* derivation &#8594; derivational\n\nThe goal of stemming and lemmatization is to reduce either inflectional forms or derivational forms of \na word to a common base form. \nBefore we demonstrate the use of several state-of-the-art stemmers and lemmatizers implemented in NLTK, please read\n[4] and section 3.6 in [2].\nIf you are a visual learner, you could watch the YouTube video on \n[Stemming](https://www.youtube.com/watch?v=2s7f8mBwnko) from Prof. Dan Jurafsky.\n\nNLTK provides several famous stemmers interfaces, such as\n\n* Porter Stemmer, which is based on \n[The Porter Stemming Algorithm](http://tartarus.org/martin/PorterStemmer/)\n* Lancaster Stemmer, which is based on \n[The Lancaster Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/),\n* Snowball Stemmer, which is based on [the Snowball Stemming Algorithm](http://snowball.tartarus.org/)\n\nLet's try the three stemmers on the words listed above.\n", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "SKZTPma-6bEz"
            }
        }, 
        {
            "source": "words = ['watches', 'parties', 'carrying', 'loving', 'stopped', 'wetter', 'fattest', \n          'dying', 'darkness', 'agreement', 'friendship', 'derivational', 'denied',  'meeting']", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "ukMvAMG46bFG", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Porter Stemming Algorithm is the one of the most common stemming algorithms.\nIt makes use of a series of heuristic replacement rules.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "BgaIIrNu6bFW"
            }
        }, 
        {
            "source": "from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "dgNgyugE6bFZ", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The Porter Stemmer works quite well on general cases, like 'watches' &#8594; 'watch' and 'darkness' &#8594; 'dark'.\nHowever, for some special cases, the Porter Stemmer might not work as expected, \nlike  'carrying'  &#8594; 'carri' and 'derivational' &#8594; 'deriv'. \nNote that a concept called \"list comprehension\" supported by Python is used here.\nIf you would like to know more about list comprehension, please click [here](http://www.secnetix.de/olli/Python/list_comprehensions.hawk).\n\nThe Lancaster Stemmer is much newer than the Porter Stemmer, published in 1990.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "5glx9qR76bFj"
            }
        }, 
        {
            "source": "from nltk.stem import LancasterStemmer\nstemmer = LancasterStemmer()\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "w6tqQde_6bFo", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "After comparing the output from the Lancaster Stemmer and that from the Porter Stemmer, you might think that\nthe Lancaster Stemmer could be a bit more aggressive than the Porter Stemmer, since it gets 'agreement' &#8594; 'agr' and 'derivational' &#8594; 'der'. \nAt the same time, it seems that the Lancaster Stemmer can handle words like 'parties' and 'carrying' quite well.\n\nNow let's try the Snowball Stemmer.\nThe version in NLTK is available in 15 languages.\nDifferent from the previous two stemmers, you need to specify which language the Snowball Stemmer will be applied to in its class constructor.\nIt works in a similar way to the Porter Stemmer.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "Jo6VCDD46bF9"
            }
        }, 
        {
            "source": "from nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "FBV_FV7y6bGR", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A stemmer usually resorts to language-specific rules. \nDifferent stemmers implementing different rules and behave differently, \nas shown above.\nThe use of inflection and derivation is very complex in English.\nThere might not exist a set of rules that can cover all the cases.\nTherefore, the stemmers that you have played will always generate some out-of-vocabulary words.\n\nRather than using a stemmer, you can use a lemmatizer that utilises\nmore information about the language to accurately identify the lemma\nfor each word.\nAs pointed out in \"**Stemming and lemmatization**\" (Please read the   related Reading Materials on the Part 4), \n> Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words\n\nThe WordNet lemmatizer implemented in NLTK is based on WordNet's built-in morphologic function, and returns the input word unchanged if it cannot be found in WordNet, which sounds more reasonable\nthan just chopping off prefixes and suffixes. In NLTK, you can use it in the following way:", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "hpZc2ZM26bGa"
            }
        }, 
        {
            "source": "from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n['{0} -> {1}'.format(w, lemmatizer.lemmatize(w)) for w in words]", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "FIGTEBI16bGf", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is a bit strange that the lemmatizer did nothing to nearly all the words, except for 'watches', 'parties'\nHowever, if we specify the POS tag of each word, what will happen?\nLet try a couple of words in our list.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "kzbZuj8G6bG5"
            }
        }, 
        {
            "source": "lemmatizer.lemmatize('dying', pos='v')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "gCUQdQ7k6bHE", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "lemmatizer.lemmatize('meeting', pos='v')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "Fkb5sN6g6bHd", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "lemmatizer.lemmatize('meeting', pos='n')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "sEtrMIGt6bIK", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "lemmatizer.lemmatize('wetter', pos='a')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "SFbhdbxD6bId", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "lemmatizer.lemmatize('fattest', pos='a')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "_54mOzQc6bIl", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "If we know the POS tags of the words, the WordNet Lemmatizer can accurately identify the corresponding lemmas.\nFor example, the word 'meeting' with different POS tag, the WordNet Lemmatizer gives you different lemmas.\nWithout giving the POS tags, it uses noun as default.\n\nBoth stemming and lemmatization can significantly reduce the number of words in a vocabulary.\nIn other words, the downstream text analysis tools can benefit from them by saving running time\nand memory space. In contrast, can stemming and lemmatization improve the performance\nof those tools? It is a quite arguable question. \nAs pointed out in [4], stemming and lemmatization can increase recall but harm precision in information\nretrieval. Researchers have also found that classifying English document tasks often do not gain \nfrom stemming and lemmatization.\nHowever, it might not be the case when we change our language to something rather than English, for example, German.", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "RJl_Da4Z6bIz"
            }
        }, 
        {
            "source": "### 2.5. Sentence Segmentation\n\nSentence segmentation is also known as sentence boundary disambiguation or sentence boundary detection.\nThe following is the Wikipedia definition of sentence boundary disambiguation:\n>Sentence boundary disambiguation (SBD), also known as sentence breaking, is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. However sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address - not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang.\n\nSBD is one of the essential problems for many NLP tasks, like Parsing, Information Extraction, Machine Translation, and Document Summarizations. \nThe accuracy of the SBD system will directly affect the performance of these applications. \n\nSentences are the basic textual unit immediately above the word and phrase. \nSo what is a sentence? Is something ending with one of the following punctuations \".\", \"!\", \"?\"?\nDoes a period always indicate sentence boundaries?\nFor English texts, it is almost as easy as finding every occurrence of those punctuations.\nHowever, some periods occur as part of abbreviations, monetary numerals and percentages, as we \nhave discussed in sections 1.2 and 1.3. \nAlthough you can use a few heuristic rules to correctly\nidentify the majority of sentence boundaries, SBD is much more complex that we can expect,\nplease read section 4.2.4 of the book, 'Corpus-Based Work'  refered on the Part 4 Reading Materials, and watch a Youtube video on [Sentence segmentation](https://www.youtube.com/watch?v=9LXq3oQEEIA). \ndiscussing more advanced techniques for SBD goes beyond our scope.\nInstead, we will show you some sentence segmentation tools implemented in NLTK.\nPlease also note that there are other tools or packages containing a sentence tokenizer,\nfor example, Apache OpenNLP, Stanford NLP toolkit, and so on.\n\nThe NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) was designed to split \ntext into sentences \"*by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.*\u201d It contains a pre-trained sentence tokenizer for English.\nLet's test it out with a couple of examples extracted from the book, called \"Moby Dick\", on Project Gutenberg, by \nHerman Melville.\nFirst construct a pre-trained English sentence tokenizer,", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "2Pfsl5v46bI6"
            }
        }, 
        {
            "source": "import nltk.data\nsent_detector = nltk.data.load('tokenizers/punkt/english.pickle')", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "N0JP3rNx6bJD", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Following the intruction on the official website of Punkt Sentence Tokenizer, we tokenize two snippets extracted\nfrom \"Moby Dick\":", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "gbC3vCn76bJQ"
            }
        }, 
        {
            "source": "text1 = '''And so it turned out; Mr. Hosea Hussey being from home, but leaving \nMrs. Hussey entirely competent to attend to all his affairs. Upon making known our desires \nfor a supper and a bed, Mrs. Hussey, postponing further scolding for the present, ushered us \ninto a little room, and seating us at a table spread with the relics of a recently concluded repast, \nturned round to us and said\u2014\"Clam or Cod?\"'''\n\n\n#('\\n-----\\n' is used to wrap the sentences after the stripped results, it is useful for\n#reading the processed the text)\nprint('\\n-----\\n'.join(sent_detector.tokenize(text1.strip())))", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "sPakoHAG6bJY", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "text2 = '''A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\nthat's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"'''\nprint('\\n-----\\n'.join(sent_detector.tokenize(text2.strip())))", 
            "cell_type": "code", 
            "metadata": {
                "colab_type": "code", 
                "id": "3Gpu_ug56bJh", 
                "colab": {}
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can also use `sent_tokenize`, an instance of Punkt Sentence Tokenizer.\nThis instance has already been trained on and works well for many European languages.\n```python\n    from nltk.tokenize import sent_tokenize\n    sent_tokenize(text1)\n```\nYou should get similar outputs as above.\n\nComparing the two results we notice that the sentence tokenizer has troubles in recognizing abbreviations.\nIt got \"Mrs.\" right in the first snippet but not the second. Regarding this type of issues, please read a blog post on sentence tokenizer , just click the 'Testing out the NLTK sentence tokenizer' on the Part 4 Reading Materials. \n* * *", 
            "cell_type": "markdown", 
            "metadata": {
                "colab_type": "text", 
                "id": "9SoAsA-26bJ0"
            }
        }, 
        {
            "source": "## Part 3. Sumary\n\nIn this chapter we have covered the fundamentals of text pre-processing. \nYou have learnt how to access different text data, and how to carry out \nthe following basic text pre-processing steps:\n* Tokenization\n* Case normalization\n* Stopping\n* Stemming and lemmatization\n* Sentence segmentation\n\nNow you should be able to perform those pre-processing tasks on a new corpus according\nto requirements of different text analysis tasks. \nWe would like to point out that besides NLTK, there are other NLP tools with mixed quality, which can be used to process text data. For example, [the standford NLP group](http://nlp.stanford.edu/software/) provides a list of tools for parsing, POS tagging, Name Entity Regonition  (NER), word segmentation, tokinization, etc; \nand [Mallet](http://mallet.cs.umass.edu/) is a Java-based package for statistical natural langage processing. \n* * *\n\n## Part 4. Reading Materials\n\n1. \"[Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\" \ud83d\udcd6 .\n2. \"[Processing Row Text](http://www.nltk.org/book_1ed/ch03.html)\", chapter 3 of\nof \"Natural Language Processing with Python\".\n3. \"[The Art of Tokenization](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)\": An IBM blog on tokenization. It gives a detailed discussion about word tokenization and its challenges \ud83d\udcd6 .\n4. \"[Stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\" \ud83d\udcd6 .\n5. \"[Dropping common terms: stop words](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\" \ud83d\udcd6 .\n6. \"[Corpus-Based Work](https://www.deakin.edu.au/library)\", Chapter 4 of \"Foundations of statistical natural language processing\" by Christopher D. Manning \ud83d\udcd6 .\n7. \"[Testing out the NLTK sentence tokenizer](http://www.robincamille.com/2012-02-18-nltk-sentence-tokenizer/)\"\n1. \"[Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper \ud83d\udcd6 .\n2. \"[Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora)\": An NLTK tutorial on accessing the contents of a diverse set of corpora.\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true, 
                "colab_type": "text", 
                "id": "cqJkbYlm6bJ3"
            }
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "colab": {
            "provenance": [
                {
                    "timestamp": 1552612501417, 
                    "file_id": "https://github.com/tulip-lab/sit742/blob/master/Jupyter/SIT742P04A-TextPreprocessing.ipynb"
                }
            ], 
            "version": "0.3.2", 
            "toc_visible": true, 
            "name": "SIT742P04A-TextPreprocessing.ipynb"
        }
    }, 
    "nbformat": 4
}